{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MICRO-ASXR MX2LM MICRONAUT AI - Colab Training\n",
        "\n",
        "Train ASX language models on Google Colab with GPU support.\n",
        "\n",
        "## Features\n",
        "- GPU-accelerated training\n",
        "- N-gram and Transformer models\n",
        "- Export models for deployment\n",
        "- Integration with ASXR Runtime Server"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Install dependencies and clone the repository."
      ],
      "metadata": {
        "id": "setup-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers datasets accelerate tqdm sentencepiece -q\n",
        "print(\"✓ Dependencies installed\")"
      ],
      "metadata": {
        "id": "install-deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "check-gpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone repository (optional - if working with remote repo)\n",
        "# !git clone https://github.com/cannaseedus-bot/MICRO-ASXR.git\n",
        "# %cd MICRO-ASXR\n",
        "\n",
        "# Or upload your files\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Create directories\n",
        "!mkdir -p brain models datasets\n",
        "print(\"✓ Directories created\")"
      ],
      "metadata": {
        "id": "setup-repo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Training Scripts\n",
        "\n",
        "Upload the ASX training scripts from your repository."
      ],
      "metadata": {
        "id": "upload-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile asx_train.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ASX Training Script for Colab\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import sys\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "class ASXDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=512):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze()\n",
        "        }\n",
        "\n",
        "def train_transformer(dataset_path, output_dir='models/asx-gpt2', epochs=3, batch_size=4):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "        texts = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "    print(f\"Loaded {len(texts)} examples\")\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    config = GPT2Config(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        n_positions=512,\n",
        "        n_embd=256,\n",
        "        n_layer=4,\n",
        "        n_head=4\n",
        "    )\n",
        "\n",
        "    model = GPT2LMHeadModel(config)\n",
        "    model.to(device)\n",
        "\n",
        "    dataset = ASXDataset(texts, tokenizer)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=input_ids\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        print(f\"Epoch {epoch+1} - Avg Loss: {epoch_loss / len(dataloader):.4f}\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"✓ Model saved to {output_dir}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('dataset', type=str)\n",
        "    parser.add_argument('--output', type=str, default='models/asx-gpt2')\n",
        "    parser.add_argument('--epochs', type=int, default=3)\n",
        "    parser.add_argument('--batch-size', type=int, default=4)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    train_transformer(args.dataset, args.output, args.epochs, args.batch_size)"
      ],
      "metadata": {
        "id": "create-training-script"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Dataset\n",
        "\n",
        "Create or upload your training dataset."
      ],
      "metadata": {
        "id": "dataset-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: Create sample dataset\n",
        "sample_data = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog\n",
        "Machine learning is transforming artificial intelligence\n",
        "Natural language processing enables computers to understand text\n",
        "Deep learning models learn from large amounts of data\n",
        "Neural networks are inspired by the human brain\n",
        "Training AI models requires significant computational resources\n",
        "GPUs accelerate machine learning training processes\n",
        "Transfer learning allows models to leverage pre-trained knowledge\n",
        "Fine-tuning adapts models to specific tasks and domains\n",
        "Language models predict the next word in a sequence\n",
        "\"\"\".strip()\n",
        "\n",
        "with open('datasets/sample.txt', 'w') as f:\n",
        "    f.write(sample_data)\n",
        "\n",
        "print(\"✓ Sample dataset created\")\n",
        "print(f\"Lines: {len(sample_data.split(chr(10)))}\")"
      ],
      "metadata": {
        "id": "create-dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 2: Upload your own dataset\n",
        "# uploaded = files.upload()\n",
        "# for filename in uploaded.keys():\n",
        "#     !mv {filename} datasets/"
      ],
      "metadata": {
        "id": "upload-dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train N-gram Model\n",
        "\n",
        "Train a simple n-gram model (CPU-friendly, no GPU needed)."
      ],
      "metadata": {
        "id": "ngram-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "def train_ngram(dataset_path, output_dir='brain'):\n",
        "    with open(dataset_path, 'r') as f:\n",
        "        texts = f.readlines()\n",
        "\n",
        "    bigrams = defaultdict(lambda: defaultdict(int))\n",
        "    trigrams = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for text in texts:\n",
        "        tokens = text.lower().strip().split()\n",
        "\n",
        "        for i in range(len(tokens) - 1):\n",
        "            bigrams[tokens[i]][tokens[i + 1]] += 1\n",
        "\n",
        "        for i in range(len(tokens) - 2):\n",
        "            key = f\"{tokens[i]} {tokens[i + 1]}\"\n",
        "            trigrams[key][tokens[i + 2]] += 1\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    with open(f'{output_dir}/bigrams.json', 'w') as f:\n",
        "        json.dump(dict(bigrams), f, indent=2)\n",
        "\n",
        "    with open(f'{output_dir}/trigrams.json', 'w') as f:\n",
        "        json.dump(dict(trigrams), f, indent=2)\n",
        "\n",
        "    print(f\"✓ N-gram model trained\")\n",
        "    print(f\"  Bigrams: {len(bigrams)} keys\")\n",
        "    print(f\"  Trigrams: {len(trigrams)} keys\")\n",
        "\n",
        "train_ngram('datasets/sample.txt')"
      ],
      "metadata": {
        "id": "train-ngram"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Transformer Model (GPU)\n",
        "\n",
        "Train a small GPT-2 style transformer model using GPU."
      ],
      "metadata": {
        "id": "transformer-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python asx_train.py datasets/sample.txt --output models/asx-gpt2 --epochs 5 --batch-size 2"
      ],
      "metadata": {
        "id": "train-transformer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Inference\n",
        "\n",
        "Test the trained models with predictions."
      ],
      "metadata": {
        "id": "inference-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test N-gram\n",
        "def predict_ngram(text, max_results=5):\n",
        "    with open('brain/bigrams.json', 'r') as f:\n",
        "        bigrams = json.load(f)\n",
        "    with open('brain/trigrams.json', 'r') as f:\n",
        "        trigrams = json.load(f)\n",
        "\n",
        "    tokens = text.lower().split()\n",
        "    predictions = []\n",
        "\n",
        "    if len(tokens) >= 2:\n",
        "        key = f\"{tokens[-2]} {tokens[-1]}\"\n",
        "        if key in trigrams:\n",
        "            predictions = sorted(trigrams[key].items(), key=lambda x: x[1], reverse=True)[:max_results]\n",
        "\n",
        "    if not predictions and len(tokens) >= 1:\n",
        "        key = tokens[-1]\n",
        "        if key in bigrams:\n",
        "            predictions = sorted(bigrams[key].items(), key=lambda x: x[1], reverse=True)[:max_results]\n",
        "\n",
        "    return predictions\n",
        "\n",
        "test_text = \"machine learning\"\n",
        "print(f\"Input: '{test_text}'\")\n",
        "print(f\"Predictions: {predict_ngram(test_text)}\")"
      ],
      "metadata": {
        "id": "test-ngram"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Transformer\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('models/asx-gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('models/asx-gpt2')\n",
        "model.eval()\n",
        "model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def generate_text(prompt, max_length=30):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=max_length,\n",
        "            num_return_sequences=1,\n",
        "            temperature=0.8,\n",
        "            do_sample=True\n",
        "        )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "prompt = \"Machine learning\"\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"Generated: {generate_text(prompt)}\")"
      ],
      "metadata": {
        "id": "test-transformer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Models\n",
        "\n",
        "Download trained models for deployment."
      ],
      "metadata": {
        "id": "export-header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip brain directory\n",
        "!zip -r brain.zip brain/\n",
        "print(\"✓ Brain zipped\")\n",
        "\n",
        "# Zip transformer model\n",
        "!zip -r asx-gpt2.zip models/asx-gpt2/\n",
        "print(\"✓ Transformer model zipped\")\n",
        "\n",
        "# Download\n",
        "files.download('brain.zip')\n",
        "files.download('asx-gpt2.zip')"
      ],
      "metadata": {
        "id": "export-models"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy to ASXR Server\n",
        "\n",
        "Instructions for deploying models to your ASXR runtime server."
      ],
      "metadata": {
        "id": "deploy-header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```bash\n",
        "# On your server:\n",
        "\n",
        "# 1. Extract brain models\n",
        "unzip brain.zip -d /path/to/MICRO-ASXR/\n",
        "\n",
        "# 2. Extract transformer models\n",
        "unzip asx-gpt2.zip -d /path/to/MICRO-ASXR/\n",
        "\n",
        "# 3. Start ASXR server\n",
        "npx @asxr/runtime-server start\n",
        "\n",
        "# 4. Test API\n",
        "curl -X POST http://localhost:3000/api/predict \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\"text\": \"machine learning\"}'\n",
        "```"
      ],
      "metadata": {
        "id": "deploy-instructions"
      }
    }
  ]
}
